{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb78b189-fd03-4d3a-b4d2-e90865a58d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd570e4-d14a-4a98-981e-f11420508cca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m HF_HUB_DISABLE_SYMLINKS_WARNING\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m T5ForConditionalGeneration\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt5-base\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mT5Tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mt5-base\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "HF_HUB_DISABLE_SYMLINKS_WARNING=False\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51557d2e-8938-4b00-954b-b0d8a902ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'summarize: Hinsichtlich eines absoluten Niedergangs ist zu sagen, dass die USA zwar sehr reale Probleme haben, aber die amerikanische Wirtschaft dennoch hoch produktiv bleibt. Amerika liegt an erster Stelle bei den Gesamtausgaben für F&amp;E, Hochschulrankings, Nobelpreisen und auch bei Unternehmensindizes. Laut Angaben des Weltwirtschaftsforums, das letzten Monat seinen Bereicht über wirtschaftliche Wettbewerbsfähigkeit veröffentlichte, liegen die USA auf Platz fünf der wettbewerbsfähigsten Ökonomien der Welt (hinter den kleinen Volkswirtschaften der Schweiz, Schwedens, Finnlands und Singapurs). China rangiert erst auf Platz 26.'\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d580309-630c-406b-9359-43a33de144de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shepa\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_ids = model.generate(input_ids)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9817d6-8db3-4e24-9a5d-52a667236b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angaben des Weltwirtschaftsforums: die USA sind auf Platz fünf der wettbewerbs'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33db87f5-7c4e-454b-89c8-86e391f68331",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.labeled\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f59f66-3bc0-4b96-b34f-ac7c9a3750f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text.split(\"\\n\\n\")\n",
    "splitted = [sentence.split(\"\\nEnglish:\\n\") for sentence in sentences]\n",
    "german_sents = [sentence[0][8:] for sentence in splitted if sentence[0] != \"\"]\n",
    "english_sents = [sentence[1] for sentence in splitted if sentence[0] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd43278-dbd7-4d88-9882-cf5a9c6a925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80d4142-f967-4b79-8312-d88517fcdba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angaben des Weltwirtschaftsforums: die USA sind auf Platz fünf der wettbewerbs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "output_ids = model.generate(input_ids)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dcb4fd-b4e2-41c7-a4d8-8c79724a67cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Wirtschaftskrise scheint die naheliegendste Erklärung zu sein, vielleicht zu naheliegend.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    model.train()\n",
    "    input_ids = tokenizer(f\"translate German to English: {german_sents[i]}\", return_tensors=\"pt\").input_ids\n",
    "    labels = tokenizer(english_sents[i], return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # the forward function automatically creates the correct decoder_input_ids\n",
    "    loss = model(input_ids=input_ids, labels=labels).loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    model.eval()\n",
    "    output_ids = model.generate(input_ids, max_length=100000)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if i % 10 == 0:\n",
    "        print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87993f91-59d4-4d69-ba7a-0dae117efb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have the following 2 training examples:\n",
    "input_sequence_1 = \"Welcome to NYC\"\n",
    "output_sequence_1 = \"Bienvenue à NYC\"\n",
    "\n",
    "input_sequence_2 = \"HuggingFace is a company\"\n",
    "output_sequence_2 = \"HuggingFace est une entreprise\"\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"translate German to English: \"\n",
    "input_sequences = [input_sequence_1, input_sequence_2]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    [output_sequence_1, output_sequence_2],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# forward pass\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}